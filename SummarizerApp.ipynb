{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Page: \n",
      "Text Summarization Techniques: A Brief Survey Mehdi Allahyari Computer Science Department University of Georgia Athens, GA mehdi@uga.edu Seyedamin Pouriyeh Computer Science Department University of Georgia Athens, GA pouriyeh@uga.edu Mehdi Assefi Computer Science Department University of Georgia Athens, GA asf@uga.edu Saeid Safaei Computer Science Department University of Georgia Athens, GA ssa@uga.edu Elizabeth D. Trippe Institute of Bioinformatics University of Georgia Athens, GA edt37727@uga.edu Juan B. Gutierrez Department of Mathematics Institute of Bioinformatics University of Georgia Athens, GA jgutierr@uga.edu Krys Kochut Computer Science Department University of Georgia Athens, GA kochut@cs.uga.edu \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Summary: \n",
      " 2 EXTRACTIVE SUMMARIZATION As mentioned before, extractive summarization techniques produce summaries by choosing a subset of the sentences in the original text. In topic representation approaches, the score of a sentence represents how well the sentence explains some of the most important topics of the text. 3.1 Topic Words The topic words technique is one of the common topic representation approaches which aims to identify words that describe the topic of the input document. This is because many of the existing text summarization techniques do not consider the semantics of words. develop a classification function, naive-Bayes classifier, to classify the sentences as summary sentences and non-summary sentences based on the features they have, given a training set of documents and their extractive summaries. Some of the frequent features used in summarization include the position of sentences in the document, sentence length, presence of uppercase words, similarity of the sentence to the document title, etc. 3.4 Bayesian Topic Models Many of the existing multi-document summarization methods have two limitations [77]: 1) They consider the sentences as independent of each other, so topics embedded in the documents are disregarded. 4 KNOWLEDGE BASES AND AUTOMATIC SUMMARIZATION The goal of automatic text summarization is to create summaries that are similar to human-created summaries. In this review, the main approaches to automatic text summarization are described. Graph-based methods and machine learning techniques are often employed to determine the important sentences to be included in the summary. 6 INDICATOR REPRESENTATION APPROACHES Indicator representation approaches aim to model the representation of the text based on a set of features and use them to directly rank the sentences rather than representing the topics of the input text. Automatic text summarization is the task of producing a concise and fluent summary while preserving key information content and overall meaning\n"
     ]
    }
   ],
   "source": [
    "#Note: \n",
    "#PDF encoding kept causing extreme data loss post conversion to txt, \n",
    "#hence the data was converted to txt outside of this python program\n",
    "#This version has been included in the git repository for ease of access alongside its .pdf counterpart\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx   \n",
    "#from nltk.stem import PorterStemmer \n",
    "\n",
    "#Recieves the location of a research paper as a .txt file \n",
    "def read_paper(file):\n",
    "  \n",
    "    #text=convert_pdf_to_txt(file)\n",
    "\n",
    "    #filedata = open(\"convertedFile.txt\", \"a\")\n",
    "    #filedata.writelines(text)\n",
    "\n",
    "    #filedata.close()\n",
    "\n",
    "    filedata = open(file, \"r\", encoding=\"utf8\")\n",
    "   \n",
    "    #Removing\n",
    "    paper_without_coverpage = filedata.read().replace('\\n', ' ').split(\"ABSTRACT\")\n",
    "    print(\"Title Page: \\n\" + paper_without_coverpage[0])\n",
    "\n",
    "    #Removing Acknowledgements and bibliography\n",
    "    paper_short = paper_without_coverpage[1].split(\"ACKNOWLEDGMENTS\") \n",
    "    \n",
    "    #Separating sentences i.e. paper is a list of sentences\n",
    "    paper = paper_short[0].split(\". \")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in paper:\n",
    "        \n",
    "        #remove whitespace\n",
    "        #sentence=sentence.strip()\n",
    "            \n",
    "        #remove header\n",
    "        sentence=sentence.replace(\"Text Summarization Techniques: A Brief Survey arXiv, July 2017, USA\",\" \")\n",
    "        \n",
    "        #remove non-ascii and digits\n",
    "        sentence=sentence.replace(\"(\\\\W|\\\\d)\",\" \")\n",
    "        \n",
    "        #[^a-zA-Z] matches to all strings that contain a symbol that is not a letter\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \"\").split(\" \"))     \n",
    "        \n",
    "        # remove markup\n",
    "        sentence=sentence.replace(\"[.*?]\",\"\")\n",
    "            \n",
    "        #print('\\n'+sentence)\n",
    "\n",
    "#Note: Stemmer commented out since stemming an entire article is a lot of manual work \n",
    "#and I feel there must be a more convenient way to approach this - but that is outside of my knowledge scope at this time\n",
    "#This is also why Lemmatization was not attempted\n",
    "    \n",
    "        #porter_stemmer=PorterStemmer()    \n",
    "        \n",
    "        #words=[\"summaries\", \"summary\", \"summarize\", \"summarization\", \"summerizers\"]\n",
    "        #stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "        #stemdf= pd.DataFrame({'original_word': words,'stemmed_word': stemmed_words})\n",
    "        \n",
    "    sentences.pop() \n",
    "    return sentences\n",
    "\n",
    "#Using cosine distance instead of euclidian since it accounts for similarity without being affected by word repetition\n",
    "def find_sentence_similarity_with_cosine_similarity(sentence1, sentence2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    #lowercasing\n",
    "    sentence1 = [w.lower() for w in sentence1]\n",
    "    sentence2 = [w.lower() for w in sentence2]\n",
    "\n",
    " \n",
    "    all_words = list(set(sentence1 + sentence2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sentence1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sentence2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "#The similarity matrix will help account for the sentences that are too similar to each other since there may be different sentences expressing the same message\n",
    "def create_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = find_sentence_similarity_with_cosine_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "# Using textrank under assumption only one article will be considered and not multiple articles - in which case Lex ranking could be usedbv \n",
    "def create_summary(file, n): #n signifies the number of first best ranked sentences to be used to create a summary\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    sentences =  read_paper(file)\n",
    "    #Generates a similarity matrix accross sentences\n",
    "    sentence_similarity_martix = create_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    #Ranks sentences in the similarity martix with a pre built pagerank algorithm\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph, max_iter=2000) #Max iteration increased since it is only 100 by default\n",
    "\n",
    "    #Sorting the resulting ranks and selecting the top n sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    \n",
    "    #print(\"Indexes of top ranked sentence order:  \", ranked_sentence)    \n",
    "        \n",
    "    for i in range(n):\n",
    "      summarize_text.append(\" \".join(ranked_sentence[i][1])) #joining the sentences into a single body of text\n",
    "\n",
    "    print(\"\\n--------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"\\nSummary: \\n\", \". \".join(summarize_text))\n",
    "\n",
    "# test\n",
    "#12 is 5% of the total sentences in this paper - after removing the acknowledgements and bibliography hence it is a fairly low compression ratio\n",
    "create_summary( \"C:/Users/Administrator/Downloads/TextSummarization.txt\", 12) \n",
    "\n",
    "#The precision ratio is hard to estimate, but from reading the generated summary, there is a lot of room for improvement. This remains the case even with higher compression ratios i.e. 10%\n",
    "#create_summary( \"C:/Users/Administrator/Downloads/TextSummarization.txt\", 24) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
